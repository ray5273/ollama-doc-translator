# Developer Guide

This document serves as a guide for developers who wish to understand and contribute to the internal structure of Ollama Document Translator.

## Project Structure

```
ollama-doc-translator/
â”œâ”€â”€ action.yml              # GitHub Action ë©”íƒ€ë°ì´í„°
â”œâ”€â”€ entrypoint.py          # ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ Dockerfile             # Docker ì»¨í…Œì´ë„ˆ ì •ì˜
â”œâ”€â”€ translate-local.py     # ë¡œì»¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ examples/              # ì‚¬ìš© ì˜ˆì œ
â”‚   â”œâ”€â”€ basic-usage.yml
â”‚   â””â”€â”€ advanced-usage.yml
â”œâ”€â”€ docs/                  # í•œêµ­ì–´ ë¬¸ì„œ
â””â”€â”€ README.md             # í”„ë¡œì íŠ¸ ë¬¸ì„œ
```

## Core Components

### 1. GitHub Action Definition (action.yml)

Defines the metadata for an Action available on the GitHub Marketplace:

```yaml
name: 'Ollama Korean to English Translator'
description: 'ë¡œì»¬ Ollama APIë¥¼ ì‚¬ìš©í•œ í•œì˜ ë²ˆì—­'
inputs:
  source-dir:
    description: 'ë²ˆì—­í•  í•œêµ­ì–´ ë¬¸ì„œ ë””ë ‰í† ë¦¬'
    default: 'docs'
outputs:
  translated-files:
    description: 'ë²ˆì—­ëœ íŒŒì¼ ìˆ˜'
```

### 2. Main Execution Logic (entrypoint.py)

Python script responsible for the core logic of the Action:

```python
def main():
    # 1. í™˜ê²½ ë³€ìˆ˜ ì½ê¸°
    # 2. Ollama ì„œë²„ ì—°ê²° í™•ì¸
    # 3. ëª¨ë¸ ê°€ìš©ì„± í™•ì¸
    # 4. ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²€ìƒ‰
    # 5. ë²ˆì—­ ì²˜ë¦¬
    # 6. PR ìƒì„±
```

### 3. Docker Container (Dockerfile)

Provides an isolated environment to execute actions:

```dockerfile
FROM python:3.11-slim
# Ollama, GitHub CLI, Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY entrypoint.py /entrypoint.py
ENTRYPOINT ["python", "/entrypoint.py"]
```

## API Design

### Ollama API Interface

```python
def translate_with_ollama(text, model="exaone3.5:7.8b"):
    """
    Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë²ˆì—­
    
    Args:
        text (str): ë²ˆì—­í•  í•œêµ­ì–´ í…ìŠ¤íŠ¸
        model (str): ì‚¬ìš©í•  Ollama ëª¨ë¸ëª…
        
    Returns:
        str: ë²ˆì—­ëœ ì˜ì–´ í…ìŠ¤íŠ¸
    """
    payload = {
        "model": model,
        "prompt": f"ë‹¤ìŒì„ ì˜ì–´ë¡œ ë²ˆì—­: {text}",
        "stream": False
    }
    response = requests.post(f"{OLLAMA_URL}/api/generate", json=payload)
    return response.json()['response']
```

### File Processing Pipeline

1. **File Discovery**: Search for markdown files using glob patterns
2. **Content Segmentation**: Divide large files into chunks using smart splitting
3. **Translation Processing**: Sequentially translate each chunk
4. **Result Merging**: Reassemble translated chunks using smart joining
5. **File Saving**: Save the translated content to the target directory

## Smart Chunking System

### Chunking Strategy Overview

The system employs a hierarchical chunking strategy to efficiently process large documents:

```python
def split_markdown_by_sections(content: str, max_tokens: int = None) -> list:
    """ì„¹ì…˜ ê¸°ë°˜ ë§ˆí¬ë‹¤ìš´ ë¶„í•  - ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´"""
    # 1. í—¤ë”© ê³„ì¸µ êµ¬ì¡° ë¶„ì„ (H1-H6)
    # 2. ì½”ë“œ ë¸”ë¡ ìƒíƒœ ì¶”ì  (``` ~ ``` ë³´ì¡´)
    # 3. í† í° ì œí•œ ë‚´ì—ì„œ ì˜ë¯¸ ë‹¨ìœ„ ìœ ì§€
    # 4. ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ë³´ì¡´ (ìƒìœ„ í—¤ë”© ê²½ë¡œ)
```

### Core Features

#### 1. Section Recognition Splitting
- **Heading Hierarchy**: H1-H2 always act as splitting boundaries, H3 split if over 200 tokens
- **Semantic Preservation**: Even small sections are maintained independently for completeness
- **Context Tracking**: Each chunk retains information about the parent heading path

#### 2. Code Block Preservation
```python
# ì½”ë“œ ë¸”ë¡ ê°ì§€ ë° ë³´ì¡´ ë¡œì§
if line_stripped.startswith('```'):
    if not in_code_block:
        in_code_block = True
        code_block_fence = line_stripped[:3]
    elif line_stripped.startswith(code_block_fence):
        in_code_block = False
        
# ì½”ë“œ ë¸”ë¡ ë‚´ë¶€ì—ì„œëŠ” ë¶„í• í•˜ì§€ ì•ŠìŒ
if not in_code_block and should_split_here:
    # ì²­í¬ ë¶„í•  ì‹¤í–‰
```

#### 3. Smart Join (Smart Join)
Prevent unnecessary line breaks when reassembling translated chunks:

```python
def smart_join_chunks(chunks: list) -> str:
    """ì—°ì†ëœ ë²ˆí˜¸ ëª©ë¡ ì‚¬ì´ì˜ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì œê±°"""
    # ë²ˆí˜¸ ëª©ë¡ íŒ¨í„´ ê°ì§€: "- 288. í•­ëª©"
    # ì—°ì† ë²ˆí˜¸ì‹œ ë‹¨ì¼ ì¤„ë°”ê¿ˆ ì‚¬ìš©
    # ì¼ë°˜ ë‚´ìš©ì€ ê¸°ë³¸ ë¶„ë¦¬ì ì‚¬ìš©
```
```

### Token Calculation System

#### Precise Token Calculation
```python
def count_tokens(text: str) -> int:
    """ì–¸ì–´ë³„ íŠ¹ì„± ê³ ë ¤í•œ í† í° ê³„ì‚°"""
    try:
        # tiktoken ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (ì„ í˜¸)
        return len(tiktoken.encoding_for_model("gpt-3.5-turbo").encode(text))
    except:
        # í´ë°±: ì–¸ì–´ë³„ ì¶”ì •
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        code_chars = len(re.findall(r'[`{}()[\];]', text))
        other_chars = len(text) - korean_chars - code_chars
        
        return int(korean_chars * 0.5 + code_chars * 0.8 + other_chars * 0.3)
```

#### Safety Margin Calculation
```python
def calculate_safe_input_tokens(context_length: int) -> int:
    """ë²ˆì—­ í”„ë¡¬í”„íŠ¸ì™€ ì¶œë ¥ ë²„í¼ ê³ ë ¤í•œ ì•ˆì „ í† í° ìˆ˜"""
    prompt_overhead = 200  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì§€ì‹œì‚¬í•­
    output_reserve = int(context_length * 0.4)  # ì¶œë ¥ ê³µê°„ 40%
    safety_margin = 100    # ì¶”ê°€ ì•ˆì „ ë§ˆì§„
    
    return context_length - prompt_overhead - output_reserve - safety_margin
```

## Debug System

### Automatic Debug File Generation

```python
# ì²­í‚¹ ë””ë²„ê·¸ íŒŒì¼
def save_debug_chunks(input_path: str, chunks: list):
    """ì²­í¬ë³„ ë¶„ì„ íŒŒì¼ ìƒì„±"""
    for i, chunk in enumerate(chunks):
        # debug_chunks/filename_chunk_001.md
        metadata = f"""<!-- DEBUG CHUNK {i+1}/{len(chunks)} -->
<!-- Tokens: {count_tokens(chunk)} -->
<!-- Characters: {len(chunk)} -->
<!-- Source: {input_path} -->"""
```

### Translation Comparison System

```python
def save_debug_translation(input_path: str, chunk_index: int, 
                         original_chunk: str, translated_chunk: str):
    """ì›ë³¸-ë²ˆì—­ ë¹„êµ íŒŒì¼ ìƒì„±"""
    # debug_originals/filename_original_001.md
    # debug_translations/filename_translated_001.md  
    # debug_comparisons/filename_comparison_001.md
```

### Enable Debug Mode

Control detailed debug information output via environment variables:

```bash
# ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”
export INPUT_DEBUG_MODE=true

# ì‹¤í–‰ì‹œ ì¶”ê°€ ì¶œë ¥:
# ğŸ“¦ Created 15 token-aware chunks
# ğŸ”„ [1/15] Translating chunk (245 tokens)...
# ğŸ› Saved debug files for chunk 1 (original/translated/comparison)
```

## Setting Up the Development Environment

### Local Development Environment

1. **Install Required Tools**:
   ```bash
   # Python ì˜ì¡´ì„±
   pip install requests
   
   # Ollama ì„¤ì¹˜
   curl -fsSL https://ollama.com/install.sh | sh
   
   # í…ŒìŠ¤íŠ¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
   ollama pull exaone3.5:7.8b
   ```

2. **ê°œë°œìš© ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰**:
   ```bash
   # ë¡œì»¬ í…ŒìŠ¤íŠ¸
   python translate-local.py
   
   # Docker í…ŒìŠ¤íŠ¸
   docker build -t ollama-translator .
   docker run --network host ollama-translator
   ```

### í…ŒìŠ¤íŠ¸ í™˜ê²½

```python
# test_translation.py
import unittest
from unittest.mock import patch, Mock

class TestTranslation(unittest.TestCase):
    @patch('requests.post')
    def test_translate_with_ollama(self, mock_post):
        # Mock API response
        mock_response = Mock()
        mock_response.json.return_value = {'response': 'Hello World'}
        mock_post.return_value = mock_response
        
        # Test translation function
        result = translate_with_ollama("ì•ˆë…•í•˜ì„¸ìš”")
        self.assertEqual(result, "Hello World")
```

## Extensibility

### Adding New Languages

To support additional language pairs, modify the following:

```python
def get_translation_prompt(text, source_lang="ko", target_lang="en"):
    prompts = {
        ("ko", "en"): f"ë‹¤ìŒ í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­: {text}",
        ("ko", "ja"): f"ë‹¤ìŒ í•œêµ­ì–´ë¥¼ ì¼ë³¸ì–´ë¡œ ë²ˆì—­: {text}",
        ("en", "ko"): f"Translate the following English to Korean: {text}"
    }
    return prompts.get((source_lang, target_lang))
```

### Supporting New File Formats

Currently, only Markdown is supported, but other formats can be added:

```python
def process_file(file_path):
    extension = file_path.suffix.lower()
    
    if extension == '.md':
        return process_markdown_file(file_path)
    elif extension == '.rst':
        return process_rst_file(file_path)
    elif extension == '.tex':
        return process_latex_file(file_path)
```

### Improving Translation Quality

1. **Prompt Engineering**:
   ```python
   def create_context_aware_prompt(text, context=""):
       return f"""
       Context: {context}
       
       Please translate the following technical document into English while maintaining:
       - Markdown format
       - Accuracy of specialized terminology first
       - Natural English expression
       
       Original: {text}
       Translation:
       """
   ```

2. **Post-Processing Improvements**:
   ```python
   def post_process_translation(translated_text):
       # Restore Markdown formatting
       translated_text = fix_markdown_formatting(translated_text)
       
       # Verify consistency of specialized terminology
       translated_text = apply_terminology_rules(translated_text)
       
       return translated_text
   ```

## Performance Optimization

### Asynchronous Processing

```python
import asyncio
import aiohttp

async def translate_async(session, text):
    async with session.post(f"{OLLAMA_URL}/api/generate", 
                           json=payload) as response:
        result = await response.json()
        return result['response']

async def process_files_async(file_list):
    async with aiohttp.ClientSession() as session:
        tasks = [translate_async(session, content) 
                for content in file_list]
        return await asyncio.gather(*tasks)
```

### Caching System

```python
import hashlib
import pickle
from pathlib import Path

class TranslationCache:
    def __init__(self, cache_dir=".translation_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    def get_cache_key(self, text, model):
        content = f"{text}:{model}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, text, model):
        cache_file = self.cache_dir / f"{self.get_cache_key(text, model)}.pkl"
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def set(self, text, model, translation):
        cache_file = self.cache_dir / f"{self.get_cache_key(text, model)}.pkl"
        with open(cache_file, 'wb') as f:
            pickle.dump(translation, f)
```

## Contribution Guide

### Coding Style

Coding standards used in the project:

```python
# PEP 8 ì¤€ìˆ˜
# í•¨ìˆ˜ëª…: snake_case
# í´ë˜ìŠ¤ëª…: PascalCase
# ìƒìˆ˜: UPPER_CASE

def translate_text(source_text: str, model_name: str) -> str:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.
    
    Args:
        source_text: ë²ˆì—­í•  ì›ë³¸ í…ìŠ¤íŠ¸
        model_name: ì‚¬ìš©í•  ëª¨ë¸ëª…
        
    Returns:
        ë²ˆì—­ëœ í…ìŠ¤íŠ¸
        
    Raises:
        TranslationError: ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ë°œìƒ
    """
    pass
```

### Commit Message Guidelines

```
feat: ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€
fix: ë²„ê·¸ ìˆ˜ì •
docs: ë¬¸ì„œ ìˆ˜ì •
style: ì½”ë“œ ìŠ¤íƒ€ì¼ ë³€ê²½
refactor: ì½”ë“œ ë¦¬íŒ©í† ë§
test: í…ŒìŠ¤íŠ¸ ì½”ë“œ ì¶”ê°€
chore: ê¸°íƒ€ ì‘ì—…

ì˜ˆì‹œ:
feat: ì¼ë³¸ì–´ ë²ˆì—­ ì§€ì› ì¶”ê°€
fix: ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹ ë³´ì¡´ ë¬¸ì œ í•´ê²°
docs: API ì‚¬ìš©ë²• ì˜ˆì œ ì¶”ê°€
```

### Pull Request Process

1. **Issue Creation**: Create an issue before developing a new feature or bug fix
2. **Branch Creation**: Use `feature/ê¸°ëŠ¥ëª…` or `fix/ë²„ê·¸ëª…` format
3. **Code Writing**: Include test code
4. **PR Creation**: Create a pull request with detailed description
5. **Review Process**: Merge after code review

### Test Writing

```python
# tests/test_translation.py
def test_korean_to_english_translation():
    """í•œêµ­ì–´-ì˜ì–´ ë²ˆì—­ í…ŒìŠ¤íŠ¸"""
    korean_text = "ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤."
    expected_english = "Hello. Nice to meet you."
    
    result = translate_with_ollama(korean_text)
    
    # ì •í™•í•œ ë²ˆì—­ì€ ì•„ë‹ˆë”ë¼ë„ í•©ë¦¬ì ì¸ ê²°ê³¼ì¸ì§€ í™•ì¸
    assert "hello" in result.lower()
    assert len(result) > 0

def test_markdown_preservation():
    """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë³´ì¡´ í…ŒìŠ¤íŠ¸"""
    markdown_text = "# ì œëª©\n\n**êµµì€ ê¸€ì”¨** ì…ë‹ˆë‹¤."
    
    result = translate_with_ollama(markdown_text)
    
    assert result.startswith("#")
    assert "**" in result
```

### Document Updates

When adding new features, ensure the following documents are updated:

- `README.md`: Basic Usage
- `action.yml`: New Input/Output Parameters
- `docs/`: Detailed Guide Documentation
- `examples/`: Usage Examples

## Deployment Process

### Version Control

Use Semantic Versioning:

- `MAJOR`: API changes that are incompatible
- `MINOR`: Addition of features compatible with previous versions
- `PATCH`: Bug fixes compatible with previous versions

### Release Procedure

1. **Create Version Tag**:
   ```bash
   git tag -a v1.2.0 -m "Release v1.2.0"
   git push origin v1.2.0
   ```

2. **Create GitHub Release**:
   - Automatically builds Docker images
   - Automatically updates Marketplace listings

3. **Update Documentation**:
   - Update version information in README.md
   - Update CHANGELOG.md

Thank you for your contribution! Feel free to ask questions via issues or discussions if you have any inquiries.

---

> **âš ï¸ ì´ ë¬¸ì„œëŠ” AIë¡œ ë²ˆì—­ëœ ë¬¸ì„œì…ë‹ˆë‹¤.**
>
> **âš ï¸ This document has been translated by AI.**