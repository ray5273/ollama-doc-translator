#!/usr/bin/env python3
"""
ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© ë²ˆì—­ ìŠ¤í¬ë¦½íŠ¸
Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ docs/ í´ë”ì˜ í•œêµ­ì–´ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ docs-en/ í´ë”ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
"""

import os
import json
import requests
import time
from pathlib import Path
from token_utils import (
    estimate_token_count,
    calculate_safe_chunk_size,
    protect_markdown_elements,
    restore_protected_elements,
    split_markdown_content,
    normalize_chunk_boundaries,
    join_translated_chunks
)

def check_ollama_server(ssl_verify=True):
    """Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5, verify=ssl_verify)
        return response.status_code == 200
    except:
        return False

def check_model_available(model="exaone3.5:7.8b", ssl_verify=True):
    """ì§€ì •ëœ ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5, verify=ssl_verify)
        if response.status_code == 200:
            models = response.json()
            model_names = [m['name'] for m in models.get('models', [])]
            return model in model_names
        return False
    except:
        return False

def translate_with_ollama(text, model="exaone3.5:7.8b", ssl_verify=True, retries=0, max_retries=3, context_length=32768):
    """Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­ (í–¥ìƒëœ ë²„ì „)"""
    url = "http://localhost:11434/api/generate"
    
    if retries >= max_retries:
        print(f"âš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries}) ë„ë‹¬, ì›ë³¸ ë°˜í™˜")
        return text, 0, 0
    
    # ë³´í˜¸ëœ ìš”ì†Œë“¤ ì²˜ë¦¬
    protected_text, protected_elements = protect_markdown_elements(text)
    
    # ì…ë ¥ í† í° ìˆ˜ ê³„ì‚°
    input_tokens = estimate_token_count(protected_text)
    
    prompt = f"""ë‹¤ìŒ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ì§€ì¹¨ì„ ì—„ê²©íˆ ë”°ë¥´ì„¸ìš”:

- ë§ˆí¬ë‹¤ìš´ í˜•ì‹ê³¼ êµ¬ì¡°ë¥¼ ì •í™•íˆ ìœ ì§€í•˜ì„¸ìš” 
- ì½”ë“œ ë¸”ë¡, ì¸ë¼ì¸ ì½”ë“œ, ë§í¬, URL, ì´ë¯¸ì§€ ê²½ë¡œ, ìˆ˜ì‹, Mermaid, HTML ì£¼ì„ì€ ë²ˆì—­í•˜ì§€ ë§ˆì„¸ìš”
- ì…ë ¥ì´ ì´ë¯¸ ì˜ì–´ì´ê±°ë‚˜ í•œêµ­ì–´ê°€ ì—†ë‹¤ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš” 
- ëª©ë¡/í…Œì´ë¸” êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”. YAML í”„ë¡ íŠ¸ ë§¤í„°ê°€ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš” 
- ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ê³  ì¶”ê°€ì ì¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”

í•œêµ­ì–´ í…ìŠ¤íŠ¸:
{protected_text}

ì˜ì–´ ë²ˆì—­:"""
    
    # í–¥ìƒëœ ì˜µì…˜ ì„¤ì •
    options = {
        "temperature": 0.2,  # ë” ì¼ê´€ëœ ë²ˆì—­ì„ ìœ„í•´ ë‚®ì¶¤
        "top_p": 0.9,
        "repeat_penalty": 1.1,
        "num_predict": -1
    }
    
    # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì„¤ì •
    if context_length > 0:
        options["num_ctx"] = context_length
    
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": options
    }
    
    try:
        print(f"ë²ˆì—­ ì¤‘... (ëª¨ë¸: {model}, í† í°: {input_tokens})")
        response = requests.post(url, json=payload, timeout=300, verify=ssl_verify)
        response.raise_for_status()
        result = response.json()
        translated = result.get('response', '').strip()
        
        # ê°€ë” ëª¨ë¸ì´ ì¶”ê°€ ì„¤ëª…ì„ í¬í•¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì •ë¦¬
        if translated.startswith('ì˜ì–´ ë²ˆì—­:'):
            translated = translated.replace('ì˜ì–´ ë²ˆì—­:', '').strip()
        
        # ë³´í˜¸ëœ ìš”ì†Œë“¤ ë³µì›
        translated = restore_protected_elements(translated, protected_elements)
        
        # ì¶œë ¥ í† í° ìˆ˜ ê³„ì‚°
        output_tokens = estimate_token_count(translated)
        
        return translated, input_tokens, output_tokens
    except Exception as e:
        print(f"ë²ˆì—­ ì˜¤ë¥˜ (ì‹œë„ {retries + 1}): {e}")
        if retries < max_retries - 1:
            time.sleep(2 ** retries)  # ì§€ìˆ˜ ë°±ì˜¤í”„
            return translate_with_ollama(text, model, ssl_verify, retries + 1, max_retries, context_length)
        else:
            return text, input_tokens, 0

def process_markdown_file(input_path, output_path, context_length=4096, ssl_verify=True):
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ í–¥ìƒëœ í† í° ê¸°ë°˜ ë°©ì‹ìœ¼ë¡œ ë²ˆì—­í•˜ì—¬ ì €ì¥"""
    print(f"\në²ˆì—­ ì¤‘: {input_path} -> {output_path}")
    
    with open(input_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ì „ì²´ ë¬¸ì„œì˜ í† í° ìˆ˜ ê³„ì‚°
    total_tokens = estimate_token_count(content)
    print(f"ğŸ“Š ë¬¸ì„œ í† í° ìˆ˜: {total_tokens}")
    
    if context_length > 0:
        # ì•ˆì „í•œ ì²­í¬ í¬ê¸° ê³„ì‚° (í† í° ê¸°ë°˜)
        max_chunk_tokens = calculate_safe_chunk_size(context_length, 0.2)
        
        if total_tokens > max_chunk_tokens:
            # ìŠ¤ë§ˆíŠ¸ ë¶„í•  ì‹¤í–‰
            print(f"ğŸ“‹ ì²­í¬ë¡œ ë¶„í•  (ìµœëŒ€ ì²­í¬ë‹¹ í† í°: {max_chunk_tokens})")
            chunks = split_markdown_content(content, max_chunk_tokens)
            chunks = normalize_chunk_boundaries(chunks)
            
            translated_chunks = []
            total_chunks = len(chunks)
            total_input_tokens = 0
            total_output_tokens = 0
            
            print(f"ğŸ“Š {total_chunks}ê°œ ì²­í¬ ì²˜ë¦¬ ì¤‘ (ì»¨í…ìŠ¤íŠ¸: {context_length})...")
            
            for i, chunk in enumerate(chunks):
                chunk_tokens = estimate_token_count(chunk)
                print(f"ğŸ”„ [{i+1}/{total_chunks}] ì²­í¬ ì²˜ë¦¬ ì¤‘ ({chunk_tokens} í† í°)...", end='')
                
                translated_chunk, input_tokens, output_tokens = translate_with_ollama(
                    chunk, ssl_verify=ssl_verify, context_length=context_length
                )
                
                if translated_chunk:
                    translated_chunks.append(translated_chunk)
                    total_input_tokens += input_tokens
                    total_output_tokens += output_tokens
                    print(f" âœ… ì™„ë£Œ ({input_tokens}â†’{output_tokens} í† í°)")
                else:
                    print(f" âš ï¸ ë¹ˆ ê²°ê³¼")
                    translated_chunks.append(chunk)  # ì›ë³¸ ì‚¬ìš©
                
                time.sleep(0.5)  # API ì†ë„ ì œí•œ ì™„í™”
            
            print(f"ğŸ“ {len(translated_chunks)}ê°œ ì²­í¬ ê²°í•© ì¤‘...")
            print(f"ğŸ“Š ì´ í† í° ì²˜ë¦¬: {total_input_tokens} â†’ {total_output_tokens}")
            translated_content = join_translated_chunks(translated_chunks)
        else:
            # íŒŒì¼ì´ ì¶©ë¶„íˆ ì‘ì•„ì„œ í•œ ë²ˆì— ì²˜ë¦¬ ê°€ëŠ¥
            print(f"ğŸ“„ ì „ì²´ íŒŒì¼ì„ í•œ ë²ˆì— ë²ˆì—­ ({total_tokens} í† í°, ì œí•œ: {max_chunk_tokens})...")
            translated_content, input_tokens, output_tokens = translate_with_ollama(
                content, ssl_verify=ssl_verify, context_length=context_length
            )
            print(f"ğŸ“Š í† í° ì²˜ë¦¬: {input_tokens} â†’ {output_tokens}")
    else:
        # Context length ì œí•œ ì—†ìŒ, ì „ì²´ íŒŒì¼ì„ í•œ ë²ˆì— ì²˜ë¦¬
        print(f"ğŸ“„ ì „ì²´ íŒŒì¼ì„ í•œ ë²ˆì— ë²ˆì—­ (ì»¨í…ìŠ¤íŠ¸ ì œí•œ ì—†ìŒ, {total_tokens} í† í°)...")
        translated_content, input_tokens, output_tokens = translate_with_ollama(
            content, ssl_verify=ssl_verify, context_length=0
        )
        print(f"ğŸ“Š í† í° ì²˜ë¦¬: {input_tokens} â†’ {output_tokens}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # ë²ˆì—­ëœ ë‚´ìš© ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(translated_content)
    
    print(f"ë²ˆì—­ ì™„ë£Œ: {output_path}")

def main():
    print("=== Ollama ë¬¸ì„œ ë²ˆì—­ê¸° ===")
    
    # ì„¤ì • ì˜µì…˜
    ssl_verify = input("SSL ì¸ì¦ì„œ ê²€ì¦ì„ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ê°’: y): ").lower() not in ['n', 'no']
    context_input = input("ëª¨ë¸ context ê¸¸ì´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (0 = chunking ì•ˆí•¨, ê¸°ë³¸ê°’: 4096): ").strip()
    context_length = int(context_input) if context_input else 4096
    
    print(f"ì„¤ì •: SSL ê²€ì¦ = {ssl_verify}, Context ê¸¸ì´ = {context_length}")
    
    # Ollama ì„œë²„ í™•ì¸
    if not check_ollama_server(ssl_verify):
        print("âŒ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ Ollamaë¥¼ ì‹œì‘í•˜ì„¸ìš”: ollama serve")
        return
    
    print("âœ… Ollama ì„œë²„ ì—°ê²°ë¨")
    
    # ëª¨ë¸ í™•ì¸
    model = "exaone3.5:7.8b"
    if not check_model_available(model, ssl_verify):
        print(f"âŒ ëª¨ë¸ '{model}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”: ollama pull {model}")
        return
    
    print(f"âœ… ëª¨ë¸ '{model}' ì‚¬ìš© ê°€ëŠ¥")
    
    docs_dir = Path('docs')
    docs_en_dir = Path('docs-en')
    
    if not docs_dir.exists():
        print("âŒ docs/ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # docs/ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì°¾ê¸°
    md_files = list(docs_dir.rglob('*.md'))
    
    if not md_files:
        print("âŒ docs/ ë””ë ‰í† ë¦¬ì— ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ“„ {len(md_files)}ê°œì˜ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:")
    for md_file in md_files:
        print(f"  - {md_file}")
    
    print(f"\nğŸš€ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    for md_file in md_files:
        # ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
        rel_path = md_file.relative_to(docs_dir)
        output_file = docs_en_dir / rel_path
        
        # ì´ë¯¸ ì¡´ì¬í•˜ê³  ë” ìƒˆë¡œìš´ ê²½ìš° ê±´ë„ˆë›°ê¸°
        if output_file.exists() and output_file.stat().st_mtime > md_file.stat().st_mtime:
            print(f"â­ï¸  {md_file} ê±´ë„ˆë›°ê¸° (ë²ˆì—­ë³¸ì´ ìµœì‹ )")
            continue
        
        process_markdown_file(md_file, output_file, context_length, ssl_verify)
    
    print(f"\nğŸ‰ ëª¨ë“  ë²ˆì—­ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ë²ˆì—­ëœ íŒŒì¼ë“¤ì€ '{docs_en_dir}' ë””ë ‰í† ë¦¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()