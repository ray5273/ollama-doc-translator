# ê°œë°œì ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” Ollama ë¬¸ì„œ ë²ˆì—­ê¸°ì˜ ë‚´ë¶€ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³  ê¸°ì—¬í•˜ê³ ì í•˜ëŠ” ê°œë°œìë¥¼ ìœ„í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
ollama-doc-translator/
â”œâ”€â”€ action.yml              # GitHub Action ë©”íƒ€ë°ì´í„°
â”œâ”€â”€ entrypoint.py          # ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ Dockerfile             # Docker ì»¨í…Œì´ë„ˆ ì •ì˜
â”œâ”€â”€ translate-local.py     # ë¡œì»¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ examples/              # ì‚¬ìš© ì˜ˆì œ
â”‚   â”œâ”€â”€ basic-usage.yml
â”‚   â””â”€â”€ advanced-usage.yml
â”œâ”€â”€ docs/                  # í•œêµ­ì–´ ë¬¸ì„œ
â””â”€â”€ README.md             # í”„ë¡œì íŠ¸ ë¬¸ì„œ
```

## í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### 1. GitHub Action ì •ì˜ (action.yml)

GitHub Marketplaceì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Actionì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤:

```yaml
name: 'Ollama Korean to English Translator'
description: 'ë¡œì»¬ Ollama APIë¥¼ ì‚¬ìš©í•œ í•œì˜ ë²ˆì—­'
inputs:
  source-dir:
    description: 'ë²ˆì—­í•  í•œêµ­ì–´ ë¬¸ì„œ ë””ë ‰í† ë¦¬'
    default: 'docs'
outputs:
  translated-files:
    description: 'ë²ˆì—­ëœ íŒŒì¼ ìˆ˜'
```

### 2. ë©”ì¸ ì‹¤í–‰ ë¡œì§ (entrypoint.py)

Actionì˜ í•µì‹¬ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” Python ìŠ¤í¬ë¦½íŠ¸:

```python
def main():
    # 1. í™˜ê²½ ë³€ìˆ˜ ì½ê¸°
    # 2. Ollama ì„œë²„ ì—°ê²° í™•ì¸
    # 3. ëª¨ë¸ ê°€ìš©ì„± í™•ì¸
    # 4. ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²€ìƒ‰
    # 5. ë²ˆì—­ ì²˜ë¦¬
    # 6. PR ìƒì„±
```

### 3. Docker ì»¨í…Œì´ë„ˆ (Dockerfile)

Actionì„ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ê²©ë¦¬ëœ í™˜ê²½ì„ ì œê³µ:

```dockerfile
FROM python:3.11-slim
# Ollama, GitHub CLI, Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY entrypoint.py /entrypoint.py
ENTRYPOINT ["python", "/entrypoint.py"]
```

## API ì„¤ê³„

### Ollama API ì¸í„°í˜ì´ìŠ¤

```python
def translate_with_ollama(text, model="exaone3.5:7.8b"):
    """
    Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë²ˆì—­
    
    Args:
        text (str): ë²ˆì—­í•  í•œêµ­ì–´ í…ìŠ¤íŠ¸
        model (str): ì‚¬ìš©í•  Ollama ëª¨ë¸ëª…
        
    Returns:
        str: ë²ˆì—­ëœ ì˜ì–´ í…ìŠ¤íŠ¸
    """
    payload = {
        "model": model,
        "prompt": f"ë‹¤ìŒì„ ì˜ì–´ë¡œ ë²ˆì—­: {text}",
        "stream": False
    }
    response = requests.post(f"{OLLAMA_URL}/api/generate", json=payload)
    return response.json()['response']
```

### íŒŒì¼ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

1. **íŒŒì¼ ë°œê²¬**: glob íŒ¨í„´ìœ¼ë¡œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²€ìƒ‰
2. **ë‚´ìš© ë¶„í• **: ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ìœ¼ë¡œ í° íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í• 
3. **ë²ˆì—­ ì²˜ë¦¬**: ê° ì²­í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë²ˆì—­
4. **ê²°ê³¼ ë³‘í•©**: ë²ˆì—­ëœ ì²­í¬ë“¤ì„ ìŠ¤ë§ˆíŠ¸ ì¡°ì¸ìœ¼ë¡œ ë‹¤ì‹œ í•©ì¹˜ê¸°
5. **íŒŒì¼ ì €ì¥**: ë²ˆì—­ëœ ë‚´ìš©ì„ ëŒ€ìƒ ë””ë ‰í† ë¦¬ì— ì €ì¥

## ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì‹œìŠ¤í…œ

### ì²­í‚¹ ì „ëµ ê°œìš”

ì‹œìŠ¤í…œì€ ëŒ€ìš©ëŸ‰ ë¬¸ì„œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ê³„ì¸µì  ì²­í‚¹ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
def split_markdown_by_sections(content: str, max_tokens: int = None) -> list:
    """ì„¹ì…˜ ê¸°ë°˜ ë§ˆí¬ë‹¤ìš´ ë¶„í•  - ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´"""
    # 1. í—¤ë”© ê³„ì¸µ êµ¬ì¡° ë¶„ì„ (H1-H6)
    # 2. ì½”ë“œ ë¸”ë¡ ìƒíƒœ ì¶”ì  (``` ~ ``` ë³´ì¡´)
    # 3. í† í° ì œí•œ ë‚´ì—ì„œ ì˜ë¯¸ ë‹¨ìœ„ ìœ ì§€
    # 4. ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ë³´ì¡´ (ìƒìœ„ í—¤ë”© ê²½ë¡œ)
```

### í•µì‹¬ ê¸°ëŠ¥

#### 1. ì„¹ì…˜ ì¸ì‹ ë¶„í• 
- **í—¤ë”© ê³„ì¸µ**: H1-H2ëŠ” í•­ìƒ ë¶„í•  ê²½ê³„, H3ëŠ” 200í† í° ì´ìƒì‹œ ë¶„í• 
- **ì˜ë¯¸ ë³´ì¡´**: ì‘ì€ ì„¹ì…˜ë„ ì™„ì „ì„±ì„ ìœ„í•´ ë…ë¦½ì ìœ¼ë¡œ ìœ ì§€
- **ì»¨í…ìŠ¤íŠ¸ ì¶”ì **: ê° ì²­í¬ëŠ” ìƒìœ„ í—¤ë”© ê²½ë¡œ ì •ë³´ ë³´ìœ 

#### 2. ì½”ë“œ ë¸”ë¡ ë³´ì¡´
```python
# ì½”ë“œ ë¸”ë¡ ê°ì§€ ë° ë³´ì¡´ ë¡œì§
if line_stripped.startswith('```'):
    if not in_code_block:
        in_code_block = True
        code_block_fence = line_stripped[:3]
    elif line_stripped.startswith(code_block_fence):
        in_code_block = False
        
# ì½”ë“œ ë¸”ë¡ ë‚´ë¶€ì—ì„œëŠ” ë¶„í• í•˜ì§€ ì•ŠìŒ
if not in_code_block and should_split_here:
    # ì²­í¬ ë¶„í•  ì‹¤í–‰
```

#### 3. ìŠ¤ë§ˆíŠ¸ ì¡°ì¸ (Smart Join)
ë²ˆì—­ëœ ì²­í¬ë“¤ì„ ë‹¤ì‹œ í•©ì¹  ë•Œ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ë°©ì§€:

```python
def smart_join_chunks(chunks: list) -> str:
    """ì—°ì†ëœ ë²ˆí˜¸ ëª©ë¡ ì‚¬ì´ì˜ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì œê±°"""
    # ë²ˆí˜¸ ëª©ë¡ íŒ¨í„´ ê°ì§€: "- 288. í•­ëª©"
    # ì—°ì† ë²ˆí˜¸ì‹œ ë‹¨ì¼ ì¤„ë°”ê¿ˆ ì‚¬ìš©
    # ì¼ë°˜ ë‚´ìš©ì€ ê¸°ë³¸ ë¶„ë¦¬ì ì‚¬ìš©
```

### í† í° ê³„ì‚° ì‹œìŠ¤í…œ

#### ì •í™•í•œ í† í° ê³„ì‚°
```python
def count_tokens(text: str) -> int:
    """ì–¸ì–´ë³„ íŠ¹ì„± ê³ ë ¤í•œ í† í° ê³„ì‚°"""
    try:
        # tiktoken ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (ì„ í˜¸)
        return len(tiktoken.encoding_for_model("gpt-3.5-turbo").encode(text))
    except:
        # í´ë°±: ì–¸ì–´ë³„ ì¶”ì •
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        code_chars = len(re.findall(r'[`{}()[\];]', text))
        other_chars = len(text) - korean_chars - code_chars
        
        return int(korean_chars * 0.5 + code_chars * 0.8 + other_chars * 0.3)
```

#### ì•ˆì „ ë§ˆì§„ ê³„ì‚°
```python
def calculate_safe_input_tokens(context_length: int) -> int:
    """ë²ˆì—­ í”„ë¡¬í”„íŠ¸ì™€ ì¶œë ¥ ë²„í¼ ê³ ë ¤í•œ ì•ˆì „ í† í° ìˆ˜"""
    prompt_overhead = 200  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì§€ì‹œì‚¬í•­
    output_reserve = int(context_length * 0.4)  # ì¶œë ¥ ê³µê°„ 40%
    safety_margin = 100    # ì¶”ê°€ ì•ˆì „ ë§ˆì§„
    
    return context_length - prompt_overhead - output_reserve - safety_margin
```

## ë””ë²„ê·¸ ì‹œìŠ¤í…œ

### ìë™ ë””ë²„ê·¸ íŒŒì¼ ìƒì„±

```python
# ì²­í‚¹ ë””ë²„ê·¸ íŒŒì¼
def save_debug_chunks(input_path: str, chunks: list):
    """ì²­í¬ë³„ ë¶„ì„ íŒŒì¼ ìƒì„±"""
    for i, chunk in enumerate(chunks):
        # debug_chunks/filename_chunk_001.md
        metadata = f"""<!-- DEBUG CHUNK {i+1}/{len(chunks)} -->
<!-- Tokens: {count_tokens(chunk)} -->
<!-- Characters: {len(chunk)} -->
<!-- Source: {input_path} -->"""
```

### ë²ˆì—­ ë¹„êµ ì‹œìŠ¤í…œ

```python
def save_debug_translation(input_path: str, chunk_index: int, 
                         original_chunk: str, translated_chunk: str):
    """ì›ë³¸-ë²ˆì—­ ë¹„êµ íŒŒì¼ ìƒì„±"""
    # debug_originals/filename_original_001.md
    # debug_translations/filename_translated_001.md  
    # debug_comparisons/filename_comparison_001.md
```

### ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”

í™˜ê²½ ë³€ìˆ˜ë¡œ ìƒì„¸ ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥ ì œì–´:

```bash
# ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”
export INPUT_DEBUG_MODE=true

# ì‹¤í–‰ì‹œ ì¶”ê°€ ì¶œë ¥:
# ğŸ“¦ Created 15 token-aware chunks
# ğŸ”„ [1/15] Translating chunk (245 tokens)...
# ğŸ› Saved debug files for chunk 1 (original/translated/comparison)
```

## ê°œë°œ í™˜ê²½ ì„¤ì •

### ë¡œì»¬ ê°œë°œ í™˜ê²½

1. **í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜**:
   ```bash
   # Python ì˜ì¡´ì„±
   pip install requests
   
   # Ollama ì„¤ì¹˜
   curl -fsSL https://ollama.com/install.sh | sh
   
   # í…ŒìŠ¤íŠ¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
   ollama pull exaone3.5:7.8b
   ```

2. **ê°œë°œìš© ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰**:
   ```bash
   # ë¡œì»¬ í…ŒìŠ¤íŠ¸
   python translate-local.py
   
   # Docker í…ŒìŠ¤íŠ¸
   docker build -t ollama-translator .
   docker run --network host ollama-translator
   ```

### í…ŒìŠ¤íŠ¸ í™˜ê²½

```python
# test_translation.py
import unittest
from unittest.mock import patch, Mock

class TestTranslation(unittest.TestCase):
    @patch('requests.post')
    def test_translate_with_ollama(self, mock_post):
        # Mock API ì‘ë‹µ
        mock_response = Mock()
        mock_response.json.return_value = {'response': 'Hello World'}
        mock_post.return_value = mock_response
        
        # ë²ˆì—­ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
        result = translate_with_ollama("ì•ˆë…•í•˜ì„¸ìš”")
        self.assertEqual(result, "Hello World")
```

## í™•ì¥ ê°€ëŠ¥ì„±

### ìƒˆë¡œìš´ ì–¸ì–´ ì¶”ê°€

ë‹¤ë¥¸ ì–¸ì–´ ìŒì„ ì§€ì›í•˜ë ¤ë©´ ë‹¤ìŒì„ ìˆ˜ì •:

```python
def get_translation_prompt(text, source_lang="ko", target_lang="en"):
    prompts = {
        ("ko", "en"): f"ë‹¤ìŒ í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­: {text}",
        ("ko", "ja"): f"ë‹¤ìŒ í•œêµ­ì–´ë¥¼ ì¼ë³¸ì–´ë¡œ ë²ˆì—­: {text}",
        ("en", "ko"): f"Translate the following English to Korean: {text}"
    }
    return prompts.get((source_lang, target_lang))
```

### ìƒˆë¡œìš´ íŒŒì¼ í˜•ì‹ ì§€ì›

í˜„ì¬ëŠ” ë§ˆí¬ë‹¤ìš´ë§Œ ì§€ì›í•˜ì§€ë§Œ ë‹¤ë¥¸ í˜•ì‹ë„ ì¶”ê°€ ê°€ëŠ¥:

```python
def process_file(file_path):
    extension = file_path.suffix.lower()
    
    if extension == '.md':
        return process_markdown_file(file_path)
    elif extension == '.rst':
        return process_rst_file(file_path)
    elif extension == '.tex':
        return process_latex_file(file_path)
```

### ë²ˆì—­ í’ˆì§ˆ ê°œì„ 

1. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**:
   ```python
   def create_context_aware_prompt(text, context=""):
       return f"""
       ì»¨í…ìŠ¤íŠ¸: {context}
       
       ë‹¤ìŒ ê¸°ìˆ  ë¬¸ì„œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”:
       - ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ìœ ì§€
       - ì „ë¬¸ ìš©ì–´ ì •í™•ì„± ìš°ì„ 
       - ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ í‘œí˜„
       
       ì›ë¬¸: {text}
       ë²ˆì—­:
       """
   ```

2. **í›„ì²˜ë¦¬ ê°œì„ **:
   ```python
   def post_process_translation(translated_text):
       # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë³µêµ¬
       translated_text = fix_markdown_formatting(translated_text)
       
       # ì „ë¬¸ ìš©ì–´ ì¼ê´€ì„± í™•ì¸
       translated_text = apply_terminology_rules(translated_text)
       
       return translated_text
   ```

## ì„±ëŠ¥ ìµœì í™”

### ë¹„ë™ê¸° ì²˜ë¦¬

```python
import asyncio
import aiohttp

async def translate_async(session, text):
    async with session.post(f"{OLLAMA_URL}/api/generate", 
                           json=payload) as response:
        result = await response.json()
        return result['response']

async def process_files_async(file_list):
    async with aiohttp.ClientSession() as session:
        tasks = [translate_async(session, content) 
                for content in file_list]
        return await asyncio.gather(*tasks)
```

### ìºì‹± ì‹œìŠ¤í…œ

```python
import hashlib
import pickle
from pathlib import Path

class TranslationCache:
    def __init__(self, cache_dir=".translation_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    def get_cache_key(self, text, model):
        content = f"{text}:{model}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, text, model):
        cache_file = self.cache_dir / f"{self.get_cache_key(text, model)}.pkl"
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def set(self, text, model, translation):
        cache_file = self.cache_dir / f"{self.get_cache_key(text, model)}.pkl"
        with open(cache_file, 'wb') as f:
            pickle.dump(translation, f)
```

## ê¸°ì—¬ ê°€ì´ë“œ

### ì½”ë“œ ìŠ¤íƒ€ì¼

í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì½”ë”© í‘œì¤€:

```python
# PEP 8 ì¤€ìˆ˜
# í•¨ìˆ˜ëª…: snake_case
# í´ë˜ìŠ¤ëª…: PascalCase
# ìƒìˆ˜: UPPER_CASE

def translate_text(source_text: str, model_name: str) -> str:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤.
    
    Args:
        source_text: ë²ˆì—­í•  ì›ë³¸ í…ìŠ¤íŠ¸
        model_name: ì‚¬ìš©í•  ëª¨ë¸ëª…
        
    Returns:
        ë²ˆì—­ëœ í…ìŠ¤íŠ¸
        
    Raises:
        TranslationError: ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ë°œìƒ
    """
    pass
```

### ì»¤ë°‹ ë©”ì‹œì§€ ê·œì¹™

```
feat: ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€
fix: ë²„ê·¸ ìˆ˜ì •
docs: ë¬¸ì„œ ìˆ˜ì •
style: ì½”ë“œ ìŠ¤íƒ€ì¼ ë³€ê²½
refactor: ì½”ë“œ ë¦¬íŒ©í† ë§
test: í…ŒìŠ¤íŠ¸ ì½”ë“œ ì¶”ê°€
chore: ê¸°íƒ€ ì‘ì—…

ì˜ˆì‹œ:
feat: ì¼ë³¸ì–´ ë²ˆì—­ ì§€ì› ì¶”ê°€
fix: ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹ ë³´ì¡´ ë¬¸ì œ í•´ê²°
docs: API ì‚¬ìš©ë²• ì˜ˆì œ ì¶”ê°€
```

### Pull Request í”„ë¡œì„¸ìŠ¤

1. **ì´ìŠˆ ìƒì„±**: ìƒˆë¡œìš´ ê¸°ëŠ¥ì´ë‚˜ ë²„ê·¸ ìˆ˜ì • ì „ ì´ìŠˆ ìƒì„±
2. **ë¸Œëœì¹˜ ìƒì„±**: `feature/ê¸°ëŠ¥ëª…` ë˜ëŠ” `fix/ë²„ê·¸ëª…` í˜•ì‹
3. **ì½”ë“œ ì‘ì„±**: í…ŒìŠ¤íŠ¸ ì½”ë“œ í¬í•¨
4. **PR ìƒì„±**: ìƒì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜
5. **ë¦¬ë·° ì§„í–‰**: ì½”ë“œ ë¦¬ë·° í›„ ë¨¸ì§€

### í…ŒìŠ¤íŠ¸ ì‘ì„±

```python
# tests/test_translation.py
def test_korean_to_english_translation():
    """í•œêµ­ì–´-ì˜ì–´ ë²ˆì—­ í…ŒìŠ¤íŠ¸"""
    korean_text = "ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤."
    expected_english = "Hello. Nice to meet you."
    
    result = translate_with_ollama(korean_text)
    
    # ì •í™•í•œ ë²ˆì—­ì€ ì•„ë‹ˆë”ë¼ë„ í•©ë¦¬ì ì¸ ê²°ê³¼ì¸ì§€ í™•ì¸
    assert "hello" in result.lower()
    assert len(result) > 0

def test_markdown_preservation():
    """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë³´ì¡´ í…ŒìŠ¤íŠ¸"""
    markdown_text = "# ì œëª©\n\n**êµµì€ ê¸€ì”¨** ì…ë‹ˆë‹¤."
    
    result = translate_with_ollama(markdown_text)
    
    assert result.startswith("#")
    assert "**" in result
```

### ë¬¸ì„œ ì—…ë°ì´íŠ¸

ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì¶”ê°€í•  ë•ŒëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì—…ë°ì´íŠ¸:

- `README.md`: ê¸°ë³¸ ì‚¬ìš©ë²•
- `action.yml`: ìƒˆë¡œìš´ ì…ë ¥/ì¶œë ¥ íŒŒë¼ë¯¸í„°
- `docs/`: ìƒì„¸ ê°€ì´ë“œ ë¬¸ì„œ
- `examples/`: ì‚¬ìš© ì˜ˆì œ

## ë°°í¬ í”„ë¡œì„¸ìŠ¤

### ë²„ì „ ê´€ë¦¬

[Semantic Versioning](https://semver.org/) ì‚¬ìš©:

- `MAJOR`: í˜¸í™˜ë˜ì§€ ì•ŠëŠ” API ë³€ê²½
- `MINOR`: í•˜ìœ„ í˜¸í™˜ì„± ìˆëŠ” ê¸°ëŠ¥ ì¶”ê°€
- `PATCH`: í•˜ìœ„ í˜¸í™˜ì„± ìˆëŠ” ë²„ê·¸ ìˆ˜ì •

### ë¦´ë¦¬ìŠ¤ ì ˆì°¨

1. **ë²„ì „ íƒœê·¸ ìƒì„±**:
   ```bash
   git tag -a v1.2.0 -m "Release v1.2.0"
   git push origin v1.2.0
   ```

2. **GitHub Release ìƒì„±**:
   - ìë™ìœ¼ë¡œ Docker ì´ë¯¸ì§€ ë¹Œë“œ
   - Marketplace ìë™ ì—…ë°ì´íŠ¸

3. **ë¬¸ì„œ ì—…ë°ì´íŠ¸**:
   - README.mdì˜ ë²„ì „ ì •ë³´
   - CHANGELOG.md ì—…ë°ì´íŠ¸

ê°œë°œì— ì°¸ì—¬í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤! ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì´ìŠˆë‚˜ ë””ìŠ¤ì»¤ì…˜ì— ì§ˆë¬¸í•´ì£¼ì„¸ìš”.